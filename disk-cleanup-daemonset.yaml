apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: disk-cleanup
  namespace: ci
spec:
  selector:
    matchLabels:
      name: disk-cleanup
  template:
    metadata:
      labels:
        name: disk-cleanup
    spec:
      hostNetwork: true
      hostPID: true
      priorityClassName: system-cluster-critical
      containers:
      - name: cleanup
        image: registry.local:5000/alpine-git:2.36.3
        command: ["/bin/sh"]
        args:
        - -c
        - |
          echo "Starting disk cleanup on $(hostname)..."
          # Use host tools via chroot (k3s uses containerd)
          CHROOT="chroot /host"
          CRICTL="/usr/bin/crictl"
          CTR="/usr/bin/ctr"
          JOURNALCTL="/usr/bin/journalctl"

          # Remove Evicted/Completed pods (free logs/directories)
          kubectl get pods -A --field-selector status.phase==Failed 2>/dev/null | awk 'NR>1 {print $1" "$2}' | while read ns pod; do kubectl -n "$ns" delete pod "$pod" --ignore-not-found=true --grace-period=0 --force || true; done
          kubectl get pods -A --field-selector status.phase==Succeeded 2>/dev/null | awk 'NR>1 {print $1" "$2}' | while read ns pod; do kubectl -n "$ns" delete pod "$pod" --ignore-not-found=true --grace-period=0 --force || true; done

          # Containerd image/prune via crictl on host (remove non-local images)
          if $CHROOT test -x $CRICTL; then
            echo "Pruning containerd images via crictl..."
            $CHROOT $CRICTL images -o table | awk 'NR>1 {print $1":"$2}' | grep -v "registry.local" | while read img; do $CHROOT $CRICTL rmi -f "$img" || true; done
            $CHROOT $CRICTL image prune || true
          fi

          # Additional prune via ctr if available
          if $CHROOT test -x $CTR; then
            echo "Pruning containerd content via ctr..."
            # Remove non-local images explicitly
            for img in $($CHROOT $CTR -n k8s.io images ls -q 2>/dev/null | tr -d '\r'); do
              echo "$img" | grep -q "registry.local" || $CHROOT $CTR -n k8s.io images rm "$img" 2>/dev/null || true
            done
            $CHROOT $CTR -n k8s.io images prune --all 2>/dev/null || true
            # Best-effort content cleanup
            IDS=$($CHROOT $CTR -n k8s.io content ls -q 2>/dev/null || true)
            if [ -n "$IDS" ]; then
              for d in $IDS; do $CHROOT $CTR -n k8s.io content rm "$d" 2>/dev/null || true; done
            fi
          fi

          # Vacuum journal logs on host (keep last 2 days)
          if $CHROOT test -x $JOURNALCTL; then
            echo "Vacuuming systemd journals..."
            $CHROOT $JOURNALCTL --vacuum-time=2d || true
          fi

          # Purge stale container logs (symlinks are fine to delete)
          find /host/var/log/containers -type f -name "*.log" -mtime +2 -delete 2>/dev/null || true
          find /host/var/log/pods -type f -name "*.log" -mtime +2 -delete 2>/dev/null || true

          # Clean old k3s data directories not in use
          if [ -d /host/var/lib/rancher/k3s/data ]; then
            CURRENT=$(readlink -f /host/var/lib/rancher/k3s/data/current 2>/dev/null || true)
            for d in /host/var/lib/rancher/k3s/data/*; do
              [ "$(basename "$d")" = "current" ] && continue
              [ -n "$CURRENT" ] && [ "$(readlink -f "$d" 2>/dev/null)" = "$CURRENT" ] && continue
              echo "Removing old k3s data dir: $d"
              rm -rf "$d" 2>/dev/null || true
            done
          fi

          # Remove orphaned containerd snapshots (best-effort)
          rm -rf /host/var/lib/rancher/k3s/agent/containerd/io.containerd.content.v1.content/tmp/* 2>/dev/null || true

          # Show disk usage after cleanup
          df -h /host/var/lib/rancher/k3s || true
          df -h /host/var/lib || true

          echo "Disk cleanup completed on $(hostname)"
        securityContext:
          privileged: true
        volumeMounts:
        - name: host-root
          mountPath: /host
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "100m"
            memory: "128Mi"
      volumes:
      - name: host-root
        hostPath:
          path: /
      tolerations:
      - key: node.kubernetes.io/disk-pressure
        operator: Exists
        effect: NoSchedule
      - key: node.kubernetes.io/disk-pressure
        operator: Exists
        effect: NoExecute