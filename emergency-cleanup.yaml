apiVersion: v1
kind: Pod
metadata:
  name: emergency-cleanup
  namespace: ci
spec:
  nodeSelector:
    kubernetes.io/hostname: k3s-worker
  hostPID: true
  restartPolicy: Never
  containers:
  - name: cleanup
    image: registry.local:5000/alpine-git:2.36.3
    command: ["/bin/sh"]
    args:
    - -c
    - |
      echo "Starting emergency disk cleanup on worker node..."
      CHROOT="chroot /host"
      CRICTL="/usr/bin/crictl"
      CTR="/usr/bin/ctr"
      JOURNALCTL="/usr/bin/journalctl"

      # Delete Completed/Evicted pods to free logs
      kubectl get pods -A --field-selector status.phase==Succeeded 2>/dev/null | awk 'NR>1 {print $1" "$2}' | while read ns pod; do kubectl -n "$ns" delete pod "$pod" --ignore-not-found=true --grace-period=0 --force || true; done
      kubectl get pods -A | grep -i Evicted | awk '{print $1" "$2}' | while read ns pod; do kubectl -n "$ns" delete pod "$pod" --ignore-not-found=true --grace-period=0 --force || true; done

      # Containerd prune via crictl/ctr, remove non-local images first
      if $CHROOT test -x $CRICTL; then
        $CHROOT $CRICTL images -o table | awk 'NR>1 {print $1":"$2}' | grep -v "registry.local" | while read img; do $CHROOT $CRICTL rmi -f "$img" || true; done
        $CHROOT $CRICTL image prune || true
      fi
      if $CHROOT test -x $CTR; then
        $CHROOT $CTR -n k8s.io images prune || true
      fi

      # Vacuum journals
      if $CHROOT test -x $JOURNALCTL; then
        $CHROOT $JOURNALCTL --vacuum-time=2d || true
      fi

      # Show disk usage after cleanup
      df -h /host/var/lib/rancher/k3s || true
      df -h /host/var/lib || true

      echo "Emergency cleanup completed"
    securityContext:
      privileged: true
    volumeMounts:
    - name: host-root
      mountPath: /host
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "100m"
        memory: "128Mi"
  volumes:
  - name: host-root
    hostPath:
      path: /